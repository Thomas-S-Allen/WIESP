---
layout: simple-page
title: DEAL Shared Task
permalink: /2022/SharedTasks
---

### DEAL: Detecting Entities in the Astrophysics Literature
# Shared Task

A good amount of astrophysics research makes use of data coming from missions and facilities such as ground observatories in remote locations or space telescopes, as well as digital archives that hold large amounts of observed and simulated data. These missions and facilities are frequently named after historical figures or use some ingenious acronym which, unfortunately, can be easily confused when searching for them in the literature via simple string matching. For instance, `Planck` can refer to the person, the mission, the constant, or several institutions. Automatically recognizing entities (i.e., Named Entity Recognition or NER) such as missions or facilities would help tackle this word sense disambiguation problem.

# Task

The task consists on building a system (any strategy is valid as long as it is automatic and does not require human intervention) that is capable of identifying Named Entities in a dataset composed by full-text fragments and acknowledgements from the astrophysics literature.

# Dataset

We provide a dataset with acknowledgements and full-text fragments from the [NASA ADS](https://ui.adsabs.harvard.edu/) with manually tagged astronomical facilities and other entities of interest (e.g., celestial objects), as well as a baseline metric obtained with the [astroBERT model](https://ui.adsabs.harvard.edu/abs/2021arXiv211200590G/abstract). We provide the [full list of tags](./data/NER-Tags.json) and a [sample of the dataset](./data/NER-Sample.jsonl) in a JSON Lines formatted file, where each entry contains at least the following keys:

- `unique_id`: A unique identifier for this data sample. It must be included in the predictions generated by the participants.
- `tokens`: The list of tokens (strings) that form the text of this sample. It must be included in the predictions generated by the participants.
- `ner_tags`: The list of NER tags (in IOB2 format)
- `ner_ids`: The pre-computed list of ids corresponding ner_tags, as given by the dictionary in [JSON Tags list](./data/NER-Tags.json)

The predictions to be provided by the participants must be given in the same JSON Lines format, and they must include the same `unique_id` and `tokens` keys from the dataset, as well as the list of predicted NER tags under a `pred_ner_tags` key (see [example](https://huggingface.co/datasets/fgrezes/WIESP2022-NER/blob/main/WIESP2022-NER-DEV-sample-predictions.jsonl)).

# Evaluation & Baseline

Submissions will be scored using both the CoNLL-2000 shared task seqeval F1-Score at the entity level and [scikit-learn's Matthews correlation coefficient method](https://scikit-learn.org/stable/modules/generated/sklearn.metrics.matthews_corrcoef.html) at the token level. We also encourage authors to propose their own evaluation metrics. The baseline will be computed using the [astroBERT](https://ui.adsabs.harvard.edu/abs/2021arXiv211200590G/abstract) trained for this NER task.

# Challenge

Can a different model/architecture/approach be more successful at recognizing astronomical named entities?

Participants will have the opportunity to present their findings during the workshop and write a short paper. The best performant or interesting approaches might be invited to further collaborate with the [NASA Astrophysical Data System](https://ui.adsabs.harvard.edu/).

# Timeline

The training dataset and baseline will be released in the coming weeks.

# Contact

You can contact us at `WIESP_AACL2022 [at] softconf.com`.

